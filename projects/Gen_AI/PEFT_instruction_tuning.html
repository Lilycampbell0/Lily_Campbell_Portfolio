<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="../../styles.css">
  <title>CNN vs. ViT for Car Brand Classification</title>
</head>
<body>
  <header>
    <h1>CNN vs. ViT for Car Brand Classification</h1>

  <nav>
  <ul class="tabs">
    
    <li class="dropdown">
      <a href="../../index.html">Home</a>
    </li>
    <li class="dropdown">
      <a href="#">Generative AI </a>
      <ul class="dropdown-content">
        <li><a href="projects/Gen_AI/RAG_research_assistant.html">RAG Research Assistant</a></li>
        <li><a href="projects/Gen_AI/PEFT_instruction_tuning.html">Instruction Tuning with PEFT </a></li>
      </ul>
    </li>
  
    <li class="dropdown">
      <a href="#">Computer Vision </a>
      <ul class="dropdown-content">
        <li><a href="../Computer_Vision_Projects/cnn_vit_car_brand.html">CNN vs ViT</a></li>
        <li><a href="../Computer_Vision_Projects/TinySwin_food101_page.html">tSwin for Food Images</a></li>
        <li><a href="../Computer_Vision_Projects/yoga_pose_page.html">Yoga Pose Identifiaction</a></li>
        <li><a href="../Computer_Vision_Projects/Argonne_Labs_Bird_Detection.html">Bird Detection</a></li>
      </ul>
    </li>
    <li class="dropdown">
      <a href="#">Big Data </a>
      <ul class="dropdown-content">
        <li><a href="../Big_Data_Projects/Big_Data_Github_Analysis.html">Github Similarity Analysis</a></li>
        <li><a href="../Big_Data_Projects/big_data_project.html">project2 big data</a></li>
      </ul>
    </li>
    <li class="dropdown">
      <a href="#">Regression & Clustering</a>
      <ul class="dropdown-content">
        <li><a href="../Regression_Projects/Home_price_regression.html">Home Price Regression Analysis</a></li>
        <li><a href="../Regression_Projects/walmart_timeseries.html">Walmart Sales Forecasting</a></li>
      </ul>
    </li>
        <li class="dropdown">
      <a href="../resume.html">Resume</a>
    </li>
    
  </ul>
</nav>
</header>

  
  <main>
    <h2>   Project Goal: </h2> 
    <p>Classify images of cars by brand, and determine whether a CNN or ViT model performs better on this task</p>
    <h2>   Applications: </h2> 
      <ul> 
        <li>Law enforcement - Many scenarios when license plates are not visible or not readable to a camera, but information around car type would be useful </li>
        <li>Finding lost cars - Could be helpful for drones to identify missing or stolen parked cars </li>
      </ul>

     <h2>   Key Literature: </h2> 
    <dl>
      <dt>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, by Dosovitskiy et al., ICLR 2021</dt>
        <dd>- “Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train”</dd>
      <dt>A Convnet for the 2020s, Liu et al., Conference on Computer Vision and Pattern Recognition 2022</dt>
        <dd>- “Surprisingly, ConvNeXts, constructed entirely from standard ConvNet modules, compete favorably with Transformers in terms of accuracy, scalability and robustness across all major benchmarks.”</dd>
    </dl>
   <h2>    Dataset: </h2> 
      <p> 1,000 images scraped from google for 8 car brands, then limited to images that are at least 4 KB to filter out low quality or unapplicable images from the scrape. </p>

    <img src="vit_cnn_files/img_cars.jpg" alt="images of cars from project" width="1100" height="200">
    
  <h2>   Modeling Approach: </h2>
    <ol>
    <li> CNN and ViT without transfer learning to serve as baselines for comparison </li>
    <li> CNN with transfer learning using the Inception ResNet V2 pre-trained model </li>
    <li> Vision Transformer with transfer learning using Google’s pretrained VIT model (google/vit-base-patch16-224) </li>
    </ol>
<h2> Results: </h2>
   <ul>  
     <li> ViT with transfer learning: 76% Test Accuracy </li>
     <li> CNN with transfer learning: 75% Test Accuracy </li>
     <li> CNN without transfer learning: 24% Test Accuracy </li>
   </ul>  
  <img src="vit_cnn_files/cnn_vit_confusion.jpg" alt="confusion matricies cnn vit" width="800" height="500">

<p> I observed similar training time for each transfer learning model, and almost the same level of accuracy achieved.
This suggests that ViT models may be a viable alternative to CNNs for the task of car brand classification. 
</p>
    
<highlight_button> 
  <a href="vit_cnn_files/cnn_vit_car_brand_classification_header.html"> Full Code for This Project </a>
</highlight_button>  
    
<h2> Highlighted Code Sections: </h2>
    
  <head>
  <link rel="stylesheet"
        href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
</head>
<body>
  <pre><code class="python">
import transformers
from unsloth import FastLanguageModel
from trl import SFTTrainer
from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import DatasetDict
import pandas as pd
import numpy as np
import torch
from torch.utils.data import Dataset
from tqdm import tqdm
from datasets import load_dataset
from torch.utils.data import DataLoader
from functools import partial
from unsloth import FastLanguageModel

## Dataset Preparation
dataset = load_dataset("databricks/databricks-dolly-15k")
def format_dolly(example):
    if example['context']:
        prompt = f"### Instruction:\n{example['instruction']}\n\n### Context:\n{example['context']}"
    else:
        prompt = f"### Instruction:\n{example['instruction']}"
    return {
        "prompt": prompt,
        "output": example["response"]
    }

formatted_dataset = dataset["train"].map(format_dolly, remove_columns=dataset["train"].column_names)
formatted_dataset = formatted_dataset.shuffle(seed=42)
train_val_test = formatted_dataset.train_test_split(test_size=0.2, seed=42)
val_test = train_val_test['test'].train_test_split(test_size=0.5, seed=42)
split_dataset = DatasetDict({
    'train': train_val_test['train'],
    'validation': val_test['train'],
    'test': val_test['test']
})

 </code></pre>
</body>

  <head>
  <link rel="stylesheet"
        href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
</head>
<body>
  <pre><code class="python">

def generate_responses(model, tokenizer, prompts, max_new_tokens=100):
    model.eval()
    responses = []
    for prompt in prompts:
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=False,
                pad_token_id=tokenizer.eos_token_id,
                temperature=0.7,
            )
        decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)
        responses.append(decoded[len(prompt):].strip())
    return responses



base_model, base_tokenizer = FastLanguageModel.from_pretrained(
    model_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
    max_seq_length = 2048,
    dtype = None,
    load_in_4bit = True,
)

baseline_outputs = generate_responses(base_model, base_tokenizer, formatted_prompts)


def to_text(example):
    return {"text": f"{example['prompt']}\n{example['output']}"}

def tokenize(example):
    tokenized = tokenizer(
        example["text"],
        truncation=True,
        padding="max_length",
        max_length=2048,
    )
    tokenized["labels"] = tokenized["input_ids"].copy()
    return tokenized

train_data = small_dataset["train"].map(to_text)
tokenized_train = train_data.map(tokenize, remove_columns=train_data.column_names)

val_data = small_dataset["validation"].map(to_text)
tokenized_val = val_data.map(tokenize, remove_columns=val_data.column_names)

 </code></pre>
</body>


 </code></pre>
</body>

  <head>
  <link rel="stylesheet"
        href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
</head>
<body>
  <pre><code class="python">
    from transformers import TrainerCallback
import matplotlib.pyplot as plt

class LossPlotCallback(TrainerCallback):
    def __init__(self):
        self.train_loss = []
        self.eval_loss = []

    def on_log(self, args, state, control, logs=None, **kwargs):
        if "loss" in logs:
            self.train_loss.append((state.epoch, logs["loss"]))
        if "eval_loss" in logs:
            self.eval_loss.append((state.epoch, logs["eval_loss"]))

    def on_train_end(self, args, state, control, **kwargs):
        train_epochs, train_losses = zip(*self.train_loss)
        plt.figure(figsize=(8, 5))
        plt.plot(train_epochs, train_losses, label="Train Loss")
        if self.eval_loss:
            eval_epochs, eval_losses = zip(*self.eval_loss)
            plt.plot(eval_epochs, eval_losses, label="Validation Loss")
        plt.xlabel("Epoch")
        plt.ylabel("Loss")
        plt.title("Training and Validation Loss")
        plt.legend()
        plt.grid(True)
        plt.tight_layout()
        plt.show()

    from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir = "/content/drive/MyDrive/genai/tinyllama-dolly-finetuned",
    num_train_epochs = 2,
    per_device_train_batch_size = 4,
    gradient_accumulation_steps = 1,
    logging_steps = 10,
    save_steps = 1000,
    eval_strategy = "no",
    save_total_limit = 1,
    fp16 = True,
    push_to_hub = False,
    report_to = "none",
)

trainer3 = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = tokenized_train,
    eval_dataset = tokenized_val,
    args = training_args,   # <- now correctly passed
    dataset_text_field = None,
    max_seq_length = 2048,
    packing = False,
    callbacks = [LossPlotCallback()],
)

trainer3.train()

trainer3.save_model("/content/drive/MyDrive/genai/tinyllama-dolly-finetuned")
tokenizer.save_pretrained("/content/drive/MyDrive/genai/tinyllama-dolly-finetuned")

  </code></pre>
</body>
    
  </main>
</body>
</html>
