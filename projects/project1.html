<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="../styles.css">
  <title> Computer Vision Projects </title>
</head>
<body>
  <header>
    <h1> CNN vs. ViT for Car Image Classification</h1>
    <nav>
      <ul class="tabs">
        <li><a href="../index.html">Home</a></li>
        <li><a href="project1.html">Computer Vision Projects</a></li>
        <li><a href="project2.html">Project 2</a></li>
        <li><a href="project3.html">Project 3</a></li>
        <li><a href="resume.html">Resume</a></li>
      </ul>
    </nav>
  </header>
  <main>
    <h2>   Project Goal: </h2> 
    <p>Classify images of cars by brand, and determine whether a CNN or ViT model performs better on this task</p>
    <h2>   Applications: </h2> 
      <ul> 
        <li>Law enforcement - Many scenarios when license plates are not visible or not readable to a camera, but information around car type would be useful </li>
        <li>Finding lost cars - Could be helpful for drones to identify missing or stolen parked cars </li>
      </ul>

     <h2>   Key Literature</h2> 
    <dl>
      <dt>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, by Dosovitskiy et al., ICLR 2021</dt>
        <dd>- “Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train”</dd>
      <dt>A Convnet for the 2020s, Liu et al., Conference on Computer Vision and Pattern Recognition 2022</dt>
        <dd>- “Surprisingly, ConvNeXts, constructed entirely from standard ConvNet modules, compete favorably with Transformers in terms of accuracy, scalability and robustness across all major benchmarks.”</dd>
    </dl>
   <h2>    Dataset: </h2> 
      <p> 1,000 images scraped from google for 8 car brands, then limited to images that are at least 4 KB to filter out low quality or unapplicable images from the scrape. </p>

    <img src="vit_cnn_files/img_cars.jpg" alt="images of cars from project" width="1100" height="200">
    
  <h2>   Modeling Approach: </h2>
    <ol>
    <li> CNN and ViT without transfer learning to serve as baselines for comparison </li>
    <li> CNN with transfer learning using the Inception ResNet V2 pre-trained model </li>
    <li> Vision Transformer with transfer learning using Google’s pretrained VIT model (google/vit-base-patch16-224) </li>
    </ol>
<h2> Results: </h2>
   <ul>  
     <li> ViT with transfer learning: 76% Test Accuracy </li>
     <li> CNN with transfer learning: 75% Test Accuracy </li>
     <li> CNN without transfer learning: 24% Test Accuracy </li>
   </ul>  
  <img src="vit_cnn_files/cnn_vit_confusion.jpg" alt="confusion matricies cnn vit" width="800" height="500">

<p> I observed similar training time for each transfer learning model, and almost the same level of accuracy achieved.
This suggests that ViT models may be a viable alternative to CNNs for the task of car brand classification. 
</p>

<h2> Key Code Sections </h2>
  <head>
  <link rel="stylesheet"
        href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
</head>
<body>
  <pre><code class="python">
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import os
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.preprocessing import LabelEncoder
 </code></pre>
</body>

    
    
<highlight_button> 
  <a href="vit_cnn_files/cnn_vit_car_brand_classification.ipynb"> Full Code for This Project </a>
</highlight_button>  
      
  </main>
</body>
</html>


